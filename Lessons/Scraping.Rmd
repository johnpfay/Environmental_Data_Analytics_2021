---
title: "17: Data Scraping"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2021"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## Objectives
1. Acquire and scrape data from web sources
2. Process web-scraped data into reproducible formats

## Set up
```{r, message = FALSE}
getwd()
library(tidyverse)
library(lubridate)
library(viridis)
#install.packages("rvest")
library(rvest)
#install.packages("ggrepel")
library(ggrepel)

# Set theme
mytheme <- theme_classic() +
  theme(axis.text = element_text(color = "black"), 
        legend.position = "top")
theme_set(mytheme)
```


## Exploring the data we want to scrape

* Navigate to <https://www.ncwater.org/WUDC/app/WWATR/report>. This site lists water use reports submitted by various industries around NC.
* Click on the link to view the report for "Plant 15" facility in 2020: <https://www.ncwater.org/WUDC/app/WWATR/report/view/0004-0001/2020>
 * View the data we can scrape on this page. 
 
What we want to do first is scrape the average daily water withdrawal for a given site along with the site's "registrant" and their "facility type". Once we see how it's done with one site, we'll look at how we can iterate through many sites and grab the data we want data from each. 


### Fetching the contents
To begin, we'll fetch the contents of the web site into our coding environment using rvests `read_html()` function. 

```{r}
#Fetch the web resources from the URL
webpage <- read_html('https://www.ncwater.org/WUDC/app/WWATR/report/view/0004-0001/2020')
webpage

```

### Understanding web pages from a coding standpoint
The `read_html()` function returns a list of the two elements, <head> and <body>, that comprise a typical web page. 
 * The <head> mostly contains information and instructions on how the page should be constructed. 
 * The <body> mostly contains the content that is displayed in your browser.
 
EXERCISE: Open the source of the web page in your browser (<ctrl>-<u>).
* Do you see the <head> and <body> sections? 
* Do you see where it states the registrant? ("American & Efird, Inc.")

HTML files follow a syntax of **tags** (what falls beteween the `<` and `>`) and **values**. Tags can indicate what kind of hyper-text element it is, and they can also include **properties** such as "id" and "class" to which specific **property values** can be assigned. This is important to know at some level, because it's this system of tags, properties, and property values that allow us to target specific elements in a web page so that we can scrape the data we want. You'll also see that items are hierarchical with some items (e.g. <table> containing other items <tr>).

### Scraping the data

Now we want to scrape the data shown in the page into a coding object. The bits we want to scrape are:
* The name of the Registrant: "American & Efird, Inc." 
* The facility type: "Industrial"
* And the average water withdrawals for each month: "0.584. 0.647, 0.543, etc."

To do this we first need to install a tool on our web browser to be able to call the web text we need. The tool is called a Selector Gadget, which for Chrome can be found [here](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en). 

The selector gadget tool is useful in identifying the internal ids of the elements shown in the web page that we want to extract. Using it is a bit clumsy, often requiring a bit of mucking about, but it still makes our work a lot easier. We'll begin by using it to determine the id of the box containing the registrant of our our site. 
1. Activate the Selector Gadget tool.
2. Click on the box listing the registrant ("American & Efird, Inc."). You'll see a number of boxes in yellow, but we just want the one box in yellow.
3. Click various other boxes until only the one we want is highlighted in yellow (or green). It's ok if others are in red. This may take a bit of trial and error, and if you get too mess up, you can start over by clicking the `Clear` button in the Selector Gadget floating toolbar.

When successful, the box should display `.table tr:nth-child(1) td:nth-child(2)`. This uniquely identifies the html element containing the data we want. 

With this id known, we can extract it's value in a two-step process. First we use `html_nodes()` to grab the element using its tag. Then we extract the value associated with that element text using the `html_text()` function. 

```{r}
the_registrant <- webpage %>% 
  html_nodes(".table tr:nth-child(1) td:nth-child(2)") %>% 
  html_text()
the_registrant
```

EXERCISE:
In the code chunk below, see if you can extract the Facility Type from the web page. Then using Selector Gadget to select multiple regions, grab all the average daily withdrawal values from the web page. Tip: Hold the <ctrl> key down while clicking elements to add/subtract regions from selected regions.

```{r}
the_facility_type <- webpage %>% 
  html_nodes("tr:nth-child(2) .left~ .left+ td.left") %>% 
  html_text()
the_facility_type

the_withdrawals <- webpage %>% 
  html_nodes('.table:nth-child(7) td:nth-child(7) , .table:nth-child(7) td:nth-child(3)') %>%
  html_text()
the_withdrawals
```

### Construct a dataframe from the data & plot the values

Now that we've scraped the data we want into our coding environment, the next step is to wrangle these data into formats we can work with more easily, i.e. a dataframe. Here, we emply the techniques we've learned earlier in this class to do this and then plot the data. 
```{r}
#Create a list of months
month = format(ISOdatetime(2020,1:12,1,0,0,0),"%b")

#Create a dataframe of withdrawals
df_withdrawals <- data.frame("Month" = rep(1:12),
                             "Year" = rep(2000,12),
                             "Withdrawals_mgd" = as.numeric(the_withdrawals))

#Modify the dataframe to include the facility and date (as date object)
df_withdrawals <- df_withdrawals %>% 
  mutate(Registrant = !!the_registrant,
         Facility_type = !!the_facility_type,
         Date = my(paste(Month,"-",Year)))

#Plot 
ggplot(df_withdrawals,aes(x=Date,y=Withdrawals_mgd)) + 
  geom_line() + 
  geom_smooth(method="loess",se=FALSE) +
  labs(title = paste("2020 Water usage data for",the_registrant),
       y="Withdrawal (mgd)",
       x="Date")
```

## Streamline the process
Progress! But let's keep going. Next, we'll streamline what we've learned above in an effort to automate the process. 

This begins by setting the parameters of the page we'll be scraping to variables so we can more easily point to, say, a different facility or a different year. Then we update our other code to use these parameters to scrape the data. 

```{r streamline.the.process}
#Construct the URL
the_base_url <- 'https://www.ncwater.org/WUDC/app/WWATR/report/view'
the_facility <- '0218-0238'#'0004-0001'
the_year <- 2015
the_scrape_url <- paste0(the_base_url, '/', the_facility, '/', the_year)
print(the_scrape_url)

#Retrieve the website contents 
the_website <- read_html(the_scrape_url)

#Set the element address variables (determined in the previous step)
the_registrant_id <- '.table tr:nth-child(1) td:nth-child(2)'
the_facility_id <- 'tr:nth-child(2) .left~ .left+ td.left'
the_data_id <- '.table:nth-child(7) td:nth-child(7) , .table:nth-child(7) td:nth-child(3)'

#Scrape the data
the_registrant <- the_website %>% html_nodes(the_registrant_id) %>% html_text()
the_facility_type <- the_website %>% html_nodes(the_facility_id) %>% html_text()
the_withdrawals <- the_website %>% html_nodes(the_data_id) %>% html_text()

df_withdrawals <- data.frame("Month" = rep(1:12),
                             "Year" = rep(the_year,12),
                             "Withdrawals_mgd" = as.numeric(the_withdrawals)) %>% 
  mutate(Registrant = !!the_registrant,
         Facility_type = !!the_facility_type,
         Date = my(paste(Month,"-",Year)))

#Plot 
ggplot(df_withdrawals,aes(x=Date,y=Withdrawals_mgd)) + 
  geom_line() + 
  geom_smooth(method="loess",se=FALSE) +
  labs(title = paste(the_year,"Water usage data for",the_registrant),
       y="Withdrawal (mgd)",
       x="Date")

```

Run the above; it should produce the same result as the previous R chunk. HOWEVER, change the year variable to `2015` and re-run the chunk. Change the facility ID to `0218-0238` and run again. Now, we have a nifty little scraping tool!


## Automation step 1: Build a function
We have our code so we can fairly easily scrape any site (if we know its ID) for any year. Let's improve our code so we can automate the process more easily and perhaps scrape many years worth of data. To make this process run more easily, we'll first convert our code into a function that produces a dataframe of withdrawal data for a given year and facility ID.

```{r}

scrape.it <- function(the_year, the_facility){

  #Retrieve the website contents 
  the_website <- read_html(paste0('https://www.ncwater.org/WUDC/app/WWATR/report/view/', the_facility, '/', the_year))
  
  #Set the element address variables (determined in the previous step)
  the_registrant_id <- '.table tr:nth-child(1) td:nth-child(2)'
  the_facility_id <- 'tr:nth-child(2) .left~ .left+ td.left'
  the_data_id <- '.table:nth-child(7) td:nth-child(7) , .table:nth-child(7) td:nth-child(3)'
  
  #Scrape the data
  the_registrant <- the_website %>% html_nodes(the_registrant_id) %>% html_text()
  the_facility_type <- the_website %>% html_nodes(the_facility_id) %>% html_text()
  the_withdrawals <- the_website %>% html_nodes(the_data_id) %>% html_text()
  
  #Convert to a dataframe
  df_withdrawals <- data.frame("Month" = rep(1:12),
                               "Year" = rep(the_year,12),
                               "Withdrawals_mgd" = as.numeric(the_withdrawals)) %>% 
    mutate(Registrant = !!the_registrant,
           Facility_type = !!the_facility_type,
           Date = my(paste(Month,"-",Year)))
  
  #Pause for a moment - scraping etiquette
  #Sys.sleep(1)
  
  #Return the dataframe
  return(df_withdrawals)
}

#Run the function
the_df <- scrape.it(2018,'0004-0001')
view(the_df)

```

## Iterate using lapply to retrieve data for a set of years
```{r}
#Set the inputs to scrape years 2015 to 2020 for the site "0004-0001"
the_years = rep(2018:2020)
my_facility = '0004-0001'

#Use lapply to apply the scrape function
the_dfs <- lapply(X = the_years,
                  FUN = scrape.it,
                  the_facility=my_facility)

#Use purrr's map function
the_dfs1 <- map(the_years,scrape.it,the_facility=my_facility)

#Conflate the returned dataframes into a single dataframe
the_df <- do.call("rbind",the_dfs1)

#Plot, because it's fun and rewarding
ggplot(the_df,aes(x=Date,y=Withdrawals_mgd)) + 
  geom_line() + 
  geom_smooth(method="loess",se=FALSE) +
  labs(title = paste(the_year,"Water usage data for",the_registrant),
       y="Withdrawal (mgd)",
       x="Date")

#Box plot?
ggplot(the_df,aes(x=as.factor(Month),y=Withdrawals_mgd)) + 
  geom_boxplot()+
  labs(x = "Month")
```

## Get a list of facility id's
The main page of reports lists all the facilities, but not the ID's that we need to scrape a site. How do we get those? That takes just a bit of more advanced scraping. Here we show how you can scrape multiple tags into a list...
```{r}
#Set the web page
the_main_url <- "https://www.ncwater.org/WUDC/app/WWATR/report"

#Pull its contents into our code environment
the_main_website <- read_html(the_main_url)

#Use Selector Gadget to find the tag for all the "View Report" objects
#You should find this to be "#content a"

#Then extract all the nodes associated with this and rather than extracting them 
#as text with html_text, we extract their "html" attribute...
the_sub_urls <- data.frame("links" = the_main_website %>% html_nodes('#content a') %>% html_attr("href"))


#To extract that value we can use substring tools
#the_sub_urls <- the_sub_urls %>% 
#  mutate(facility_id = str_sub(links, start = 52, end = 60))
 # mutate(facility_id = str_split(links,pattern="/"))

the_sub_urls <- the_sub_urls %>% 
  mutate(facility_id = str_split(links,"/") %>% map_chr(.,9)) #Splits the html into items, then extracts the 9th item

```

